{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"N1AFU0WGMEfl"},"outputs":[],"source":["#TODO -- what can I modify -- parameters like different loss function, optimizer, epoch, etc??\n","# can also do another pretrained CNN or my own tiny CNN??"]},{"cell_type":"markdown","metadata":{"id":"AnoeytMwX0aH"},"source":["#Setup and Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1026,"status":"ok","timestamp":1746778660166,"user":{"displayName":"Emma O'Brien","userId":"10965466160024993297"},"user_tz":-60},"id":"Ee03HKI8XYOL","outputId":"9ad6def0-d49b-45b8-c39a-962ab18bd97f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4040,"status":"ok","timestamp":1746778664205,"user":{"displayName":"Emma O'Brien","userId":"10965466160024993297"},"user_tz":-60},"id":"n3WrzFcnX4ej","outputId":"ac1c645f-24a9-42de-91ac-d00c22ce841e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7c44a32524f0>"]},"metadata":{},"execution_count":4}],"source":["import cv2\n","import tarfile\n","import os\n","import numpy as np\n","import pandas as pd\n","import pickle\n","from tqdm import tqdm #progress bars\n","import torch\n","\n","#sampling\n","from collections import defaultdict # can group class without checking if key already exists\n","import random\n","\n","#normalize and resize\n","import torchvision.transforms as transforms\n","\n","#dataset\n","from sklearn.preprocessing import LabelEncoder\n","from torch.utils.data import Dataset\n","from sklearn.model_selection import train_test_split\n","from PIL import Image\n","from torch.utils.data import DataLoader\n","from pycocotools.coco import COCO\n","from collections import Counter\n","\n","#pretrained CNN\n","import torchvision.models as models\n","import torch.nn as nn\n","\n","#training\n","from sklearn.metrics import f1_score\n","import torch.optim as optim\n","\n","#plot\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","random.seed(42)\n","np.random.seed(42)\n","torch.manual_seed(42)"]},{"cell_type":"markdown","metadata":{"id":"82a4gsajX5Xh"},"source":["#Load iCub"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZUzTovGOXkj2"},"outputs":[],"source":["def load_icub_data(dataset_path, max_per_class=500):\n","    labels, image_paths = [], []\n","\n","    # Get all 'part*' folders (e.g., part1, part2, ...)\n","    part_folders = [f for f in os.listdir(dataset_path) if f.startswith('part')]\n","\n","    for part in part_folders:\n","        part_path = os.path.join(dataset_path, part)\n","\n","        # Loop over class folders (e.g., book, pencilcase, etc.)\n","        for class_name in os.listdir(part_path):\n","            class_path = os.path.join(part_path, class_name)\n","            if not os.path.isdir(class_path):\n","                continue\n","\n","            # Loop over object instances (e.g., book1, book2, ...)\n","            for instance_name in os.listdir(class_path):\n","                instance_path = os.path.join(class_path, instance_name)\n","\n","                if not os.path.isdir(instance_path):\n","                    continue\n","\n","                # Only use 'MIX' transformation\n","                mix_path = os.path.join(instance_path, 'MIX')\n","                if not os.path.isdir(mix_path):\n","                    continue\n","\n","                # Loop over days (e.g., day5, day6, ...)\n","                for day_name in os.listdir(mix_path):\n","                    day_path = os.path.join(mix_path, day_name)\n","                    if not os.path.isdir(day_path):\n","                        continue\n","\n","                    # Use only left camera images\n","                    left_path = os.path.join(day_path, 'left')\n","                    if not os.path.isdir(left_path):\n","                        continue\n","\n","                    # Loop through image files\n","                    for file in os.listdir(left_path):\n","                        if not file.lower().endswith(('.jpg', '.jpeg', '.png')):\n","                            continue\n","\n","                        #now record label and image path for each image\n","                        img_path = os.path.join(left_path, file)\n","                        labels.append(class_name)  # use high-level class name\n","                        image_paths.append(img_path)\n","    return sample_dataset(image_paths, labels, max_per_class)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LUK3nxQJXzir"},"outputs":[],"source":["def sample_dataset(image_paths, labels, max_per_class=500, seed=42):\n","    # Group image paths by class\n","    class_to_images = defaultdict(list)\n","    for path, label in zip(image_paths, labels):\n","        class_to_images[label].append(path)\n","\n","    # Set seed for reproducibility\n","    random.seed(seed)\n","\n","    # Subsample each class\n","    sampled_image_paths = []\n","    sampled_labels = []\n","\n","    for label, paths in class_to_images.items():\n","        random.shuffle(paths)\n","        selected = paths[:max_per_class]\n","        sampled_image_paths.extend(selected)\n","        sampled_labels.extend([label] * len(selected))\n","\n","    # shuffle data -- avoid order bias\n","    combined = list(zip(sampled_image_paths, sampled_labels))\n","    random.shuffle(combined)\n","    sampled_image_paths, sampled_labels = zip(*combined)\n","\n","    return list(sampled_image_paths), list(sampled_labels)\n"]},{"cell_type":"markdown","metadata":{"id":"XzUbJy4C1LJY"},"source":["#Load COCO"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IwfdQII51NyP"},"outputs":[],"source":["\n","#note: labels must be from 0-89 for loss function\n","def build_image_label_map(coco, top_k=20, target_count=8000):\n","    img_ids = coco.getImgIds()\n","    temp_labels = {}\n","    cat_count = Counter()\n","\n","    # most common object per image - no filtering yet\n","    for img_id in img_ids:\n","        anns = coco.loadAnns(coco.getAnnIds(imgIds=img_id))\n","        if not anns:\n","            continue\n","        cat_ids = [ann['category_id'] for ann in anns]\n","        most_common_cat = Counter(cat_ids).most_common(1)[0][0]\n","        fname = coco.loadImgs(img_id)[0]['file_name']\n","        temp_labels[fname] = most_common_cat\n","        cat_count[most_common_cat] += 1\n","\n","    #get only the top k categories\n","    top_cats = set([cat for cat, _ in cat_count.most_common(top_k)])\n","    label_to_index = {cat_id: idx for idx, cat_id in enumerate(sorted(top_cats))}\n","\n","    # get images from top_k categories only until target_count is reached\n","    image_label_map = {}\n","    class_image_counts = defaultdict(int)\n","\n","    for fname, cat_id in temp_labels.items():\n","        if cat_id in top_cats:\n","            image_label_map[fname] = label_to_index[cat_id]\n","            class_image_counts[cat_id] += 1\n","            if len(image_label_map) >= target_count:\n","                break\n","\n","    return image_label_map, label_to_index\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"129JMXrE1OlE"},"outputs":[],"source":["def paths_and_labels(image_label_map, img_dir):\n","    paths = []\n","    labels = []\n","    for fname, label in image_label_map.items():\n","        path = os.path.join(img_dir, fname)\n","        if os.path.exists(path):\n","            paths.append(path)\n","            labels.append(label)\n","    return paths, labels"]},{"cell_type":"markdown","metadata":{"id":"k2UUelXrYQSt"},"source":["#Data preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wb-_WN69YU5l"},"outputs":[],"source":["#make dataset class to feed data into pytorch\n","class ImageDataset(Dataset):\n","    def __init__(self, image_paths, labels, transform=None):\n","        self.image_paths = image_paths\n","        self.labels = labels\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, idx):\n","        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n","        label = self.labels[idx]\n","        if self.transform:\n","            image = self.transform(image)\n","        return image, label"]},{"cell_type":"markdown","metadata":{"id":"8RqCcdtXDad0"},"source":["#Load pretrained model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9w1MyT8sGobV"},"outputs":[],"source":["def build_model(num_classes, dropout=.5):\n","    #load in ResNet-18\n","    model = models.resnet18(pretrained=True)\n","\n","    # diable gradient updates for weights -- only want train the classifier part at the nd\n","    for param in model.parameters():\n","        param.requires_grad = False\n","\n","    #replace the classifier layer (fully connected linear layer), add dropout\n","    model.fc = nn.Sequential(\n","        nn.Dropout(dropout),\n","        nn.Linear(model.fc.in_features, num_classes)\n","    )\n","\n","    for name, param in model.named_parameters():\n","        if \"layer4\" in name or \"fc\" in name:\n","            param.requires_grad = True\n","\n","    return model.to(device)"]},{"cell_type":"markdown","metadata":{"id":"Yh9oKWwLKn7M"},"source":["# Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m_CTv_jYLpFO"},"outputs":[],"source":["def train_model(model, train_loader, val_loader, optimizer, scheduler, criterion, num_epochs=10, dropout=None, lr=None, optimizer_name=None, augment=None):\n","    #track best f1 score\n","    best_f1 = 0.0\n","    #starts new epoch\n","    for epoch in range(num_epochs):\n","        model.train() #TRAINING MODE\n","        running_loss, correct, total = 0.0, 0, 0\n","        #loop over batches\n","        for images, labels in tqdm(train_loader,\n","                               desc=f\"Epoch {epoch+1}/{num_epochs}\",\n","                               leave=False):\n","            images, labels = images.to(device), labels.to(device)\n","\n","            optimizer.zero_grad() #reset gradient from last batch\n","            outputs = model(images) #forward pass\n","            loss = criterion(outputs, labels)\n","            loss.backward()#backward pass\n","            optimizer.step() #update weights\n","\n","            running_loss += loss.item() #get total loss over all batches\n","            _, predicted = outputs.max(1) #get predicted class from softmax\n","            total += labels.size(0)\n","            correct += predicted.eq(labels).sum().item() #get how many predicts are correct\n","\n","        #decide learning rate\n","        scheduler.step()\n","        #evaluate how model is learning\n","        val_acc, val_f1, _, _ = evaluate_model(model, val_loader)\n","        if val_f1 > best_f1:\n","            best_f1 = val_f1\n","            #save the best model for later\n","            torch.save(model.state_dict(), f\"best_model_dropout{dropout}_lr{lr}_{optimizer_name}_aug{augment}.pth\")\n","            print(f\"new best model with F1 = {val_f1:.4f}\")\n","        print(f\"Epoch {epoch+1}/{num_epochs}, Train Acc: {100*correct/total:.2f}%, Val F1: {val_f1:.4f}\")\n","    return best_f1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8d5WLn8qMXnv"},"outputs":[],"source":["def evaluate_model(model, data_loader):\n","    model.eval()\n","    all_preds, all_labels = [], []\n","    with torch.no_grad(): #disable gradient tracking\n","        #load through validation dataset\n","        for images, labels in data_loader:\n","            images, labels = images.to(device), labels.to(device)\n","            #forward pass\n","            outputs = model(images)\n","            #compute validation accuracy\n","            _, preds = outputs.max(1)\n","            all_preds.extend(preds.cpu().numpy())\n","            all_labels.extend(labels.cpu().numpy())\n","    acc = np.mean(np.array(all_preds) == np.array(all_labels)) * 100\n","    f1 = f1_score(all_labels, all_preds, average='macro')\n","    return acc, f1, all_labels, all_preds"]},{"cell_type":"code","source":["##if the dataset is icub only, then take the best model and test it against the test set\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","import torchvision.transforms as transforms\n","\n","def evaluate_best_icub_model(test_paths, test_labels, best_config,\n","                             batch_size=32, num_workers=4):\n","    # 1) Validation‐style transforms (no randomness!)\n","    test_transform = transforms.Compose([\n","        transforms.Resize((224, 224)),\n","        transforms.ToTensor(),\n","        transforms.Normalize(\n","            mean=[0.485, 0.456, 0.406],\n","            std=[0.229, 0.224, 0.225]\n","        )\n","    ])\n","\n","    # 2) Build Dataset & DataLoader\n","    test_dataset = ImageDataset(test_paths, test_labels, transform=test_transform)\n","    test_loader  = DataLoader(\n","        test_dataset,\n","        batch_size=batch_size,\n","        shuffle=False,\n","        num_workers=num_workers,\n","        pin_memory=True\n","    )\n","\n","    # 3) Rebuild model\n","    num_classes = len(set(test_labels))\n","    model = build_model(num_classes, dropout=best_config['dropout'])\n","\n","    # 4) Load checkpoint\n","    ckpt_filename = (\n","        f\"best_model_dropout{best_config['dropout']}\"\n","        f\"_lr{best_config['lr']}_{best_config['optimizer']}\"\n","        f\"_aug{best_config.get('augment', False)}.pth\"\n","    )\n","    state = torch.load(ckpt_filename, map_location=device)\n","    model.load_state_dict(state)\n","    model.to(device)\n","    model.eval()\n","\n","    # 5) Criterion & evaluation\n","    criterion = nn.CrossEntropyLoss()\n","    test_acc, test_f1, all_labels, all_preds = evaluate_model(model, test_loader)\n","    cm = confusion_matrix(all_labels, all_preds)\n","\n","    return test_acc, test_f1, cm"],"metadata":{"id":"VuQQZBScXfyT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WrvJmK91YIi3"},"source":["#Run experiment -- with different settings (dropout, learning rate, optimizer, augemented data, number of epochs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tr-F5IAuNZDE"},"outputs":[],"source":["from torchvision.datasets import ImageFolder\n","def run_experiment(dropout, lr, optimizer_name, augment=False, num_epochs=10):\n","    if augment:\n","        train_transform = transforms.Compose([\n","            transforms.Resize((224, 224)),\n","            transforms.RandomHorizontalFlip(),\n","            transforms.ColorJitter(0.2, 0.2, 0.2),\n","            transforms.ToTensor(),\n","            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","        ])\n","        #validation set should not be augmented\n","        val_transform = transforms.Compose([\n","        transforms.Resize((224, 224)),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","])\n","    else:\n","        #resize and normalize data (basically to match pretrained model)----\n","        #must resize data to (224, 224) bc pretrained CNNS were trained on ImageNet (where all images were 224x224 pixel)\n","        #normalize to speed up convergence during training\n","        #use mean and std from ImageNet to match what network was trained on\n","        train_transform = transforms.Compose([\n","            transforms.Resize((224, 224)),\n","            transforms.ToTensor(),\n","            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","        ])\n","        #validation and training set need to have the same transformation\n","        val_transform = train_transform\n","\n","    train_dataset = ImageDataset(train_paths, train_labels, train_transform)\n","    val_dataset = ImageDataset(val_paths, val_labels, val_transform)\n","\n","    #create dataloaders -- helps feed batches into model\n","    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n","    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)\n","\n","    model = build_model(num_classes=len(label_encoder.classes_), dropout=dropout)\n","\n","    #experiment with different optimizers\n","    trainable_params = filter(lambda p: p.requires_grad, model.parameters())\n","\n","    if optimizer_name.lower() == 'adam':\n","        optimizer = optim.Adam(trainable_params, lr=lr)\n","    elif optimizer_name.lower() == 'sgd':\n","        optimizer = optim.SGD(trainable_params, lr=lr, momentum=0.9)\n","    else:\n","        raise ValueError(\"Unsupported optimizer\")\n","\n","    #experiment with different learning rates\n","    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n","    #use cross entropy loss function -- why?\n","    criterion = nn.CrossEntropyLoss()\n","\n","    best_val_f1 = train_model(\n","        model, train_loader, val_loader, optimizer, scheduler, criterion, num_epochs,\n","        dropout=dropout, lr=lr, optimizer_name=optimizer_name, augment=augment\n","    )\n","\n","    return {\n","        \"dropout\": dropout,\n","        \"lr\": lr,\n","        \"optimizer\": optimizer_name,\n","        \"augment\": augment,\n","        \"val_f1\": best_val_f1,\n","        \"model\": model,\n","        \"criterion\": criterion\n","\n","    }\n"]},{"cell_type":"markdown","metadata":{"id":"yq_raHLoOmRQ"},"source":["#Run all experiments (24)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BPO8kNgEOoVS","executionInfo":{"status":"ok","timestamp":1746781634978,"user_tz":-60,"elapsed":2970642,"user":{"displayName":"Emma O'Brien","userId":"10965466160024993297"}},"outputId":"5d332eda-9033-49dc-fa59-6fc141eda53c"},"outputs":[{"output_type":"stream","name":"stdout","text":["['/content/drive/MyDrive/U of Manchester/Robotics Assignment/data/icubworld_data/part2/hairbrush/hairbrush6/MIX/day6/left/00007137.jpg', '/content/drive/MyDrive/U of Manchester/Robotics Assignment/data/icubworld_data/part2/hairclip/hairclip1/MIX/day8/left/00000772.jpg', '/content/drive/MyDrive/U of Manchester/Robotics Assignment/data/icubworld_data/part1/mouse/mouse4/MIX/day8/left/00004080.jpg', '/content/drive/MyDrive/U of Manchester/Robotics Assignment/data/icubworld_data/part3/soapdispenser/soapdispenser3/MIX/day4/left/00002609.jpg', '/content/drive/MyDrive/U of Manchester/Robotics Assignment/data/icubworld_data/part1/pencilcase/pencilcase7/MIX/day5/left/00001671.jpg']\n","[ 5  6  7 14 10]\n","\n","['/content/drive/MyDrive/U of Manchester/Robotics Assignment/data/icubworld_data/part4/squeezer/squeezer9/MIX/day1/left/00006051.jpg', '/content/drive/MyDrive/U of Manchester/Robotics Assignment/data/icubworld_data/part1/mouse/mouse6/MIX/day7/left/00001940.jpg', '/content/drive/MyDrive/U of Manchester/Robotics Assignment/data/icubworld_data/part4/ovenglove/ovenglove8/MIX/day1/left/00002846.jpg', '/content/drive/MyDrive/U of Manchester/Robotics Assignment/data/icubworld_data/part1/mouse/mouse7/MIX/day7/left/00003301.jpg', '/content/drive/MyDrive/U of Manchester/Robotics Assignment/data/icubworld_data/part4/squeezer/squeezer3/MIX/day2/left/00002002.jpg']\n","[17  7  9  7 17]\n","Train images: 7200\n","Val images:   1800\n","Test images:  1000\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n","100%|██████████| 44.7M/44.7M [00:00<00:00, 235MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["new best model with F1 = 0.8644\n","Epoch 1/10, Train Acc: 73.26%, Val F1: 0.8644\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["new best model with F1 = 0.8672\n","Epoch 2/10, Train Acc: 92.89%, Val F1: 0.8672\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["new best model with F1 = 0.8985\n","Epoch 3/10, Train Acc: 95.99%, Val F1: 0.8985\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["new best model with F1 = 0.9233\n","Epoch 4/10, Train Acc: 97.86%, Val F1: 0.9233\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 5/10, Train Acc: 98.49%, Val F1: 0.9187\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["new best model with F1 = 0.9495\n","Epoch 6/10, Train Acc: 99.46%, Val F1: 0.9495\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["new best model with F1 = 0.9549\n","Epoch 7/10, Train Acc: 99.90%, Val F1: 0.9549\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 8/10, Train Acc: 99.93%, Val F1: 0.9544\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 9/10, Train Acc: 99.96%, Val F1: 0.9538\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["new best model with F1 = 0.9576\n","Epoch 10/10, Train Acc: 99.99%, Val F1: 0.9576\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"output_type":"stream","name":"stdout","text":["Micro-averaged Precision: 0.9577777777777777\n","Micro-averaged Recall:    0.9577777777777777\n","Macro-averaged Precision: 0.9581798674939552\n","Macro-averaged Recall:    0.9577777777777776\n"]}],"source":["# - Dropout rate: [0.0, 0.3, 0.5] → to see how regularization affects overfitting\n","# - Learning rate: [1e-3, 1e-4] → to compare fast vs slow learning\n","# - Optimizer: ['adam', 'sgd'] → to test adaptive vs momentum-based optimization\n","# - Data augmentation: [False, True] → to see if transforms help generalization\n","# In total: 3 x 2 x 2 x 2 = 24 experiments\n","\n","results = []\n","##CHANGE THIS\n","data = 'icub'\n","\n","if data == 'icub':\n","    dataset_path = \"/content/drive/MyDrive/U of Manchester/Robotics Assignment/data/icubworld_data\"\n","    #get the data\n","    sampled_image_paths, sampled_labels = load_icub_data(dataset_path, max_per_class=500)\n","    label_encoder = LabelEncoder()\n","    encoded_labels = label_encoder.fit_transform(sampled_labels)\n","\n","    #split into training, validation, testing\n","    trainval_paths, test_paths, trainval_labels, test_labels = train_test_split(\n","        sampled_image_paths, encoded_labels, test_size=0.1, stratify=encoded_labels, random_state=42)\n","    train_paths, val_paths, train_labels, val_labels = train_test_split(\n","        trainval_paths, trainval_labels, test_size=0.2, stratify=trainval_labels, random_state=42)\n","\n","elif data == 'coco':\n","    annotation_path_val = '/content/drive/MyDrive/U of Manchester/Robotics Assignment/data/coco/annotations/instances_val2017.json'\n","    annotation_path_train = '/content/drive/MyDrive/U of Manchester/Robotics Assignment/data/coco/annotations/instances_train2017.json'\n","    coco_val = COCO(annotation_path_val)\n","    coco_train = COCO(annotation_path_train)\n","\n","\n","    #coco only has train and val\n","    train_image_label_map, train_label_to_index = build_image_label_map(coco_train, target_count=8000)\n","    val_image_label_map,   val_label_to_index   = build_image_label_map(coco_val, target_count=2000)\n","\n","    all_labels = set(train_image_label_map.values()) | set(val_image_label_map.values())\n","    label_encoder = LabelEncoder()\n","    label_encoder.fit(list(sorted(all_labels)))\n","\n","    local_train_path = \"/content/train2017\"\n","    local_val_path   = \"/content/val2017\"\n","\n","    #unzip\n","    if not os.path.exists(local_train_path):\n","        print(\"Unzipping training images...\")\n","        !unzip -q \"/content/drive/MyDrive/U of Manchester/Robotics Assignment/data/train2017.zip\" -d /content/\n","    if not os.path.exists(local_val_path):\n","        print(\"Unzipping validation images...\")\n","        !unzip -q \"/content/drive/MyDrive/U of Manchester/Robotics Assignment/data/val2017.zip\" -d /content/\n","\n","    train_paths, train_labels = paths_and_labels(train_image_label_map, local_train_path)\n","    val_paths, val_labels     = paths_and_labels(val_image_label_map, local_val_path)\n","\n","    # encode them\n","    train_labels = label_encoder.transform(train_labels)\n","    val_labels  = label_encoder.transform(val_labels)\n","\n","#debug\n","print(\"Train images:\", len(train_paths))\n","print(\"Val images:  \", len(val_paths))\n","if data == 'icub':\n","    print(\"Test images: \", len(test_paths))\n","#RUN the experiments\n","for dropout in [0.0, 0.3, 0.5]:\n","    for lr in [1e-3, 1e-4]:\n","        for opt in ['adam', 'sgd']:\n","            for augment in [False, True]:\n","                print(f\"Running: dropout={dropout}, lr={lr}, optimizer={opt}, augment={augment}\")\n","                result = run_experiment(dropout, lr, opt, augment)\n","                results.append(result)\n","#save\n","df = pd.DataFrame(results)\n","df.to_csv(\"experiment_results.csv\", index=False)\n","df\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wiL3NUTWSSbh"},"outputs":[],"source":["#read in file if not already done\n","#df = pd.read_csv(\"experiment_results.csv\")\n","\n","#get the best model-- test it\n","# best_config = df.sort_values(by=\"val_f1\", ascending=False).iloc[0]\n","# print(\"Best configuration:\\n\", best_config)\n","\n","# print(best_config)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Er_Sj8eb6Lb0"},"outputs":[],"source":["# --- Usage example, right after your grid-run ---\n","best_idx    = df['val_f1'].idxmax()\n","best_config = df.loc[best_idx]\n","\n","# test_acc, test_f1, cm = evaluate_best_icub_model(\n","#     test_paths, test_labels, best_config,\n","#     batch_size=best_config.get('batch_size', 32),\n","#     num_workers=4\n","# )\n","\n","test_acc, test_f1, cm = evaluate_best_icub_model(\n","    val_paths, val_labels, best_config,\n","    batch_size=best_config.get('batch_size', 32),\n","    num_workers=4\n",")\n","\n","print(f\"Best test accuracy: {test_acc:.2f}%\")\n","print(f\"Best test   F1  : {test_f1:.4f}\")\n","\n","\n","disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n","disp.plot(xticks_rotation='vertical')\n","plt.title(\"Confusion Matrix\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3DV43AgtScOQ"},"outputs":[],"source":["import seaborn as sns\n","\n","cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","\n","plt.figure(figsize=(12, 10))\n","sns.heatmap(cm_normalized, cmap='viridis', xticklabels=False, yticklabels=False, cbar=True)\n","plt.title('Normalized Confusion Matrix')\n","plt.xlabel('Predicted label')\n","plt.ylabel('True label')\n","plt.show()\n"]},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Extract from confusion matrix\n","true_positives = np.diag(cm)\n","support = cm.sum(axis=1)       # total actual instances per class\n","predicted = cm.sum(axis=0)     # total predicted instances per class\n","\n","# Safe division: avoids division by zero\n","recall = np.divide(true_positives, support, out=np.zeros_like(true_positives, dtype=float), where=support != 0)\n","precision = np.divide(true_positives, predicted, out=np.zeros_like(true_positives, dtype=float), where=predicted != 0)\n","accuracy_per_class = recall  # same as per-class accuracy in single-label settings\n","\n","# Print summary\n","print(\"Mean per-class accuracy:\", np.mean(accuracy_per_class))\n","print(\"Mean precision:\", np.mean(precision))\n","print(\"Mean recall:\", np.mean(recall))\n","\n","# Plot per-class accuracy\n","plt.figure(figsize=(12, 4))\n","plt.plot(accuracy_per_class, label='Per-class Accuracy')\n","plt.xlabel('Class Index')\n","plt.ylabel('Accuracy')\n","plt.title('Per-class Accuracy from Confusion Matrix')\n","plt.grid(True)\n","plt.legend()\n","plt.show()\n","\n"],"metadata":{"id":"uFMktHlgYRIH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Micro-averaged: global TP, FP, FN\n","import numpy as np\n","true_positives = np.diag(cm).sum()\n","total_predicted = cm.sum(axis=0).sum()   # total predicted = total predictions\n","total_actual = cm.sum(axis=1).sum()      # total actual = total ground truth labels\n","\n","precision_micro = true_positives / cm.sum(axis=0).sum()\n","recall_micro = true_positives / cm.sum(axis=1).sum()\n","\n","print(\"Micro-averaged Precision:\", precision_micro)\n","print(\"Micro-averaged Recall:   \", recall_micro)\n"],"metadata":{"id":"FA4NROMVZokk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","true_positives = np.diag(cm).sum()\n","total_predicted = cm.sum(axis=0).sum()   # total predicted = total predictions\n","total_actual = cm.sum(axis=1).sum()      # total actual = total ground truth labels\n","\n","precision_micro = true_positives / cm.sum(axis=0).sum()\n","recall_micro = true_positives / cm.sum(axis=1).sum()\n","\n","print(\"Micro-averaged Precision:\", precision_micro)\n","print(\"Micro-averaged Recall:   \", recall_micro)\n","\n","tp_per_class = np.diag(cm)\n","support = cm.sum(axis=1)       # ground truth count per class\n","predicted = cm.sum(axis=0)     # predicted count per class\n","\n","precision_per_class = np.divide(tp_per_class, predicted, out=np.zeros_like(tp_per_class, dtype=float), where=predicted != 0)\n","recall_per_class = np.divide(tp_per_class, support, out=np.zeros_like(tp_per_class, dtype=float), where=support != 0)\n","\n","precision_macro = np.mean(precision_per_class)\n","recall_macro = np.mean(recall_per_class)\n","\n","print(\"Macro-averaged Precision:\", precision_macro)\n","print(\"Macro-averaged Recall:   \", recall_macro)\n"],"metadata":{"id":"qXyBKBOvZ7_u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"nCIvtjWxaIgq"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","machine_shape":"hm","provenance":[],"mount_file_id":"1dqiGKOM6DnrQgCYZMOdIMxfg2IbHVG7G","authorship_tag":"ABX9TyO+IjKZcMfvF+L3lkhIvlJ1"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}